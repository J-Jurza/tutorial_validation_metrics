{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresssion metrics\n",
    "\n",
    "A critical step in building a machine learning model is checking how well it performs. This is done with the use of validation metrics. In this notebook, we’ll explore essential metrics for **regression**, equipping you with the knowledge to assess your models effectively. I'll cover when to use each metric, the pros and cons and how to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple Model\n",
    "Let's build a simple model so we can demonstrate classification metrics.\n",
    "This will be a linear regresison model that is suitable with our metrics. \n",
    "Let's load the diabetes dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 152.13348416289597\n",
      "Coefficients: [ -10.0098663  -239.81564367  519.84592005  324.3846455  -792.17563855\n",
      "  476.73902101  101.04326794  177.06323767  751.27369956   67.62669218]\n"
     ]
    }
   ],
   "source": [
    "# Load the diabetes dataset from sklearn\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Create a pandas DataFrame from the diabetes data\n",
    "df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "df['target'] = diabetes.target\n",
    "\n",
    "# Define the independent and dependent variables\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Create the linear regression model\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Print the intercept and coefficient of the model\n",
    "print('Intercept:', model.intercept_)\n",
    "print('Coefficients:', model.coef_)\n",
    "\n",
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mean Absolute Error (MAE)\n",
    "Mean Absolute Error (MAE) measures the average of the absolute differences between predicted and actual values. The formula for MAE is:\n",
    "\n",
    "![Alt text](MAE.png)\n",
    "\n",
    "N is the number of data points.\n",
    "y_pred is the predicted value.\n",
    "y_test is the actual value.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- Easy to interpret: Represents average error magnitude.\n",
    "- Less sensitive to outliers than Mean Squared Error (MSE).\n",
    "- No error direction: Doesn’t indicate overestimation or underestimation.\n",
    "- May not capture extreme errors’ impact in some contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate MAE score\n",
    "mae = mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Mean Squared Error (MSE)\n",
    "\n",
    "Mean Squared Error (MSE) measures the average of the squared differences between predicted and actual values. The formula for MSE is:\n",
    "\n",
    "![Alt text](Images/MSE.png)\n",
    "\n",
    "MSE is sensitive to outliers, as it penalises large errors more heavily than small errors. This can be both an advantage and a disadvantage, depending on the problem context.\n",
    "**Summary:**\n",
    "\n",
    "- More sensitive to extreme errors.\n",
    "- Squared error values can be less intuitive than absolute errors.\n",
    "- More influenced by outliers compared to Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7977288857345636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "# Print the mean squared error\n",
    "print('Mean Squared Error:', mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Root Mean Squared Error (RMSE)\n",
    "\n",
    "Root Mean Squared Error (RMSE) is the square root of the mean squared error. The formula for RMSE is:\n",
    "\n",
    "![Alt text](Images/RMSE.png)\n",
    "\n",
    "RMSE is also sensitive to outliers, and like MSE, it penalises large errors more heavily. The advantage of RMSE over MSE is that its unit is the same as the target variable, making it easier to interpret.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- More sensitive to extreme errors.\n",
    "-  Same unit as target variable:\n",
    "-  More influenced by outliers compared to Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 53.47612876402657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print the root mean squared error\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "Mean Absolute Percentage Error (MAPE) measures the average of the absolute percentage differences between predicted and actual values. The formula for MAPE is:\n",
    "\n",
    "![Alt text](Images/MAPE.png)\n",
    "\n",
    "MAPE is useful when comparing the performance of different models or when assessing the relative importance of errors. However, MAPE can be problematic when dealing with values close to zero, as the percentage error can become extremely large.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- Relative error metric: Useful for comparing model performance across different scales.\n",
    "- Easy to interpret: Expressed as a percentage.\n",
    "- Undefined for zero values, which may occur in some applications.\n",
    "- Asymmetric: Overestimates errors for small actual values and underestimates for large ones.\n",
    "\n",
    "Scikit learn doesn’t have a MAPE function, but we can calculate it ourselves using the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error: 38.78617921794824\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the mean absolute percentage error\n",
    "mape = np.mean(np.abs((y - y_pred) / y)) * 100\n",
    "\n",
    "# Print the mean absolute percentage error\n",
    "print('Mean Absolute Percentage Error:', mape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 R-Squared (Coefficient of Determination)\n",
    "R-Squared measures the proportion of variance in the target variable that can be explained by the model’s predictions. The formula for R-Squared is:\n",
    "\n",
    "![Alt text](Images/R-Squared.png)\n",
    "\n",
    "y_mean is the mean of the actual values.\n",
    "y_pred is the predicted value.\n",
    "y_test is the actual value.\n",
    "R-Squared ranges from 0 to 1, with higher values indicating better model performance. However, R-Squared has limitations, such as the possibility of increasing with the addition of irrelevant features.\n",
    "**Summary:**\n",
    "\n",
    "- Represents the proportion of explained variance, making it easy to understand and communicate.\n",
    "- Less sensitive to the scale of the target variable, which makes it more suitable for comparing the performance of different models.\n",
    "- Biased towards models with many features, which may not always be desirable.\n",
    "- Not appropriate for evaluating models that do not have a linear relationship between the predictors and the target variable.\n",
    "- May be affected by outliers in the data.\n",
    "\n",
    "In Python, we can get R-Squared using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared: 0.5177484222203499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the R squared\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Print the R squared\n",
    "print('R Squared:', r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Adjusted R-Squared\n",
    "Adjusted R-Squared is a modified version of R-Squared that considers the number of features in the model. The formula for Adjusted R-Squared is:\n",
    "\n",
    "![Alt text](Images/adj%20r-squared.png)\n",
    "\n",
    "N is the number of data points.\n",
    "k is the number of features.\n",
    "Adjusted R-Squared can help prevent overfitting by penalising models with excessive features.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- Modification of R-squared that adjusts for the number of predictors in the model, which makes it a more appropriate metric for comparing the performance of models with different numbers of predictors.\n",
    "- Less sensitive to the scale of the target variable, which makes it more suitable for comparing the performance of different models.\n",
    "- Penalises extra variables: Reduces overfitting risk compared to R-squared.\n",
    "- Not appropriate for evaluating models that do not have a linear relationship between the predictors and the target variable.\n",
    "- May be affected by outliers in the data.\n",
    "\n",
    "In Python, we can calculate it from our R-squared score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared: 0.5065592904853232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# R-squared\n",
    "r_squared = r2_score(y, y_pred)\n",
    "\n",
    "# Adjusted R-squared\n",
    "n = len(y) # number of observations\n",
    "p = X.shape[1]  # number of predictors\n",
    "adj_r_squared = 1 - ((1 - r_squared) * (n - 1)) / (n - p - 1)\n",
    "\n",
    "print(\"Adjusted R-squared:\", adj_r_squared)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 Mean Percentage Error (MPE)\n",
    "\n",
    "The average percentage difference between the actual and predicted values of the dependent variable. \n",
    "\n",
    "It is calculated as (1/n) * sum((actual - predicted)/actual) * 100.\n",
    "\n",
    "\n",
    "**Summary:**\n",
    "- MPE is a simple and interpretable metric that can be used to understand the average magnitude of the errors as a percentage of the actual values.\n",
    "- MPE can be used to compare the accuracy of different models, as it provides a simple and interpretable metric that is not affected by the scale of the data.\n",
    "- MPE can be useful for understanding the direction and magnitude of the errors, as positive MPE values indicate that the predictions are too high on average, while negative - - MPE values indicate that the predictions are too low on average.\n",
    "- MPE can be sensitive to the presence of zero values in the data, as the percentage change from zero to a non-zero value is undefined.\n",
    "- MPE can be biased when the actual values are close to zero or when the data is heavily skewed or contains outliers.\n",
    "- MPE can give misleading results when the data contains extreme values or is not normally distributed, as the percentage error may not accurately reflect the relative size of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Percentage Error: -17.477529250120554\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the mean percentage error\n",
    "mpe = np.mean((y - y_pred) / y) * 100\n",
    "\n",
    "# Print the mean percentage error\n",
    "print('Mean Percentage Error:', mpe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8 Geometric Mean Absolute Error (GMAE)\n",
    "\n",
    "The geometric mean of the absolute errors between the actual and predicted values of the dependent variable. \n",
    "\n",
    "It is calculated as exp(1/n * sum(log(abs(actual - predicted)))).\n",
    "\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- GMAE is robust to outliers, as it takes the geometric mean of the absolute errors rather than the arithmetic mean.\n",
    "- GMAE can be useful for understanding the overall accuracy of the predictions, especially when there are large errors in the predictions.\n",
    "- GMAE can be used to compare the accuracy of different models, as it provides a simple and interpretable metric that is not affected by the scale of the data.\n",
    "- GMAE can be sensitive to the presence of zero values in the data, as the logarithm of zero is undefined.\n",
    "- GMAE is less widely used than other regression metrics, such as mean squared error or mean absolute error, which may make it harder to compare the results to other studies or to find pre-existing code for calculating it.\n",
    "- GMAE assumes that the errors of the model are symmetric and that the scale of the errors is constant across the entire dataset, which may not always be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geometric Mean Absolute Error: 29.274524510051712\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the geometric mean absolute error\n",
    "gmae = np.exp(np.mean(np.log(np.abs(y - y_pred))))\n",
    "\n",
    "# Print the geometric mean absolute error\n",
    "print('Geometric Mean Absolute Error:', gmae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.9 Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "\n",
    "A measure of forecast accuracy that is scaled by the mean absolute error of a naive forecast. \n",
    "\n",
    "It is calculated as (1/n) * sum(|actual - predicted| / MAE_naive_forecast), \n",
    "\n",
    "where MAE_naive_forecast is the mean absolute error of a naive forecast that predicts the value of the dependent variable to be the same as its value in the previous period.\n",
    "\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "- MASE can be used to compare the accuracy of different time series forecasting models on different datasets and at different forecasting horizons.\n",
    "- MASE can be used to compare the accuracy of different models without assuming that the errors of the models are normally distributed or that the data is stationary.\n",
    "- MASE provides a simple and interpretable metric that can be used to understand the accuracy of a forecast relative to a simple benchmark forecast.\n",
    "- MASE can be sensitive to the choice of the benchmark forecast used to calculate the denominator of the ratio.\n",
    "- MASE can be sensitive to the choice of the forecast horizon, as different forecast horizons may require different benchmark forecasts.\n",
    "- MASE assumes that the errors of the model are symmetric and that the scale of the errors is constant across the entire time series, which may not always be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric Mean Absolute Percentage Error: 31.40103299212728\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the symmetric mean absolute percentage error\n",
    "smape = 100/len(y) * np.sum(2 * np.abs(y - y_pred) / (np.abs(y) + np.abs(y_pred)))\n",
    "\n",
    "# Print the symmetric mean absolute percentage error\n",
    "print('Symmetric Mean Absolute Percentage Error:', smape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.10 Mean Absolute Scaled Error (MASE)\n",
    "\n",
    "A measure of forecast accuracy that is scaled by the mean absolute error of a naive forecast. \n",
    "\n",
    "It is calculated as (1/n) * sum(|actual - predicted| / MAE_naive_forecast), \n",
    "\n",
    "where MAE_naive_forecast is the mean absolute error of a naive forecast that predicts the value of the dependent variable to be the same as its value in the previous period.\n",
    "\n",
    "\n",
    "**Summary**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Scaled Error: 0.5053581619224684\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the mean absolute error of a naive forecast\n",
    "naive_forecast = y.shift(1)\n",
    "mae_naive_forecast = np.mean(np.abs(y[1:] - naive_forecast[1:]))\n",
    "\n",
    "# Calculate the mean absolute scaled error\n",
    "mase = np.mean(np.abs(y - y_pred)) / mae_naive_forecast\n",
    "\n",
    "# Print the mean absolute scaled error\n",
    "print('Mean Absolute Scaled Error:', mase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.11 Theil's U-Statistic\n",
    "A measure of the relative forecast accuracy of two models. \n",
    "\n",
    "It is calculated as (1/n) * sum((actual - predicted1) / actual) / (1/n) * sum((actual - predicted2) / actual)\n",
    "\n",
    "where predicted1 and predicted2 are the predicted values of the dependent variable from two different models.\n",
    "\n",
    "**Summary**:\n",
    "- Theil's U-Statistic is a simple and straightforward way to compare the forecast accuracy of two different models.\n",
    "- Theil's U-Statistic is easy to interpret, as a value less than 1 indicates that the first model is more accurate, and a value greater than 1 indicates that the second model is more accurate.\n",
    "- Theil's U-Statistic is robust to outliers, as it compares the relative accuracy of the two models rather than their absolute accuracy.\n",
    "- Theil's U-Statistic can be sensitive to the choice of forecast horizon, as it is calculated based on the ratio of mean squared errors rather than the absolute differences between the actual and predicted values.\n",
    "- Theil's U-Statistic assumes that the errors of the two models are identically and independently distributed, which may not always be the case.\n",
    "- Theil's U-Statistic does not provide information about the statistical significance of the difference in accuracy between the two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theil's U-Statistic: 0.998783659694973\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the first model\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Create the second linear regression model\n",
    "model2 = LinearRegression().fit(X.iloc[:, :-1], y)\n",
    "\n",
    "# Make predictions using the second model\n",
    "y_pred2 = model2.predict(X.iloc[:, :-1])\n",
    "\n",
    "# Calculate Theil's U-Statistic\n",
    "u_statistic = np.sqrt(np.mean((y - y_pred1)**2)) / np.sqrt(np.mean((y - y_pred2)**2))\n",
    "\n",
    "# Print Theil's U-Statistic\n",
    "print(\"Theil's U-Statistic:\", u_statistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_impute",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
